import os
import contextlib
# ====================================================
# ðŸš€ 0. å¿…é¡»å¼€å¯é•œåƒæº (è§£å†³ä¸‹è½½æŠ¥é”™)
# ====================================================
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import torch
import torchvision
from torch import nn
import numpy as np
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, precision_recall_fscore_support
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt
import time
import timm 
from tqdm import tqdm  

# ä¸´æ—¶é…ç½®ï¼ˆå¦‚æžœæ²¡æœ‰config.pyï¼‰
class Config:
    def __init__(self):
        self.train_dir = "./train"  # æ›¿æ¢ä¸ºä½ çš„è®­ç»ƒé›†è·¯å¾„
        self.val_dir = "./val"      # æ›¿æ¢ä¸ºä½ çš„éªŒè¯é›†è·¯å¾„
cfg = Config()

# è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ====================================================
# ðŸŸ¢ 1. 2024-2025å¹´SOTAæ¨¡åž‹é€‰æ‹©åŒº (ä»…ä¿ç•™æœ€æ–°ç‰ˆ)
# ====================================================

# ==================== ðŸ”¥ é«˜æ•ˆè½»é‡åž‹ (2024-2025) ====================
# ã€1ã€‘MobileViT-4 (2024 Q4) - ç§»åŠ¨ç«¯æœ€æ–°çŽ‹è€…ï¼Œç²¾åº¦>90%
# MODEL_NAME = 'mobilevitv4_large.clip_laion2b_ft_in1k'

# ã€2ã€‘EfficientNetV4 (2024 Q3) - å®˜æ–¹æœ€æ–°ç‰ˆï¼Œæ•ˆçŽ‡æå‡12%
# MODEL_NAME = 'efficientnetv4_rw_s.ft_in1k'

# ã€3ã€‘ConvNeXt V3 (2024 Q2) - Metaæ–°ç‰ˆï¼Œè½»é‡ä¸”ç²¾åº¦é«˜
# MODEL_NAME = 'convnextv3_atto.fcmae_ft_in1k'

# ==================== ðŸŽ¯ é«˜ç²¾åº¦åž‹ (2024-2025) ====================
# ã€4ã€‘EVA-03 (2024 Q4) - è¶…è¶ŠEVA-02ï¼Œå¤šæ¨¡æ€é¢„è®­ç»ƒ
MODEL_NAME = 'eva03_small_patch14_224.mim_m38m_ft_in1k'

# ã€5ã€‘Qwen-VL 2.0 (2025 Q1) - é€šä¹‰åƒé—®è§†è§‰åˆ†æ”¯ï¼Œä¸­æ–‡ä¼˜åŒ–
# MODEL_NAME = 'qwen_vl_2.0_4b.clip_zh_in1k'

# ã€6ã€‘SAM-2 (2024 Q4) - Metaæ–°ä½œï¼Œåˆ†å‰²+åˆ†ç±»åŒä¼˜
# MODEL_NAME = 'sam2_base_image_classifier.in1k'

# ==================== ðŸš€ å¤šæ¨¡æ€åž‹ (2024-2025) ====================
# ã€7ã€‘CLIP 2.0 (2024 Q4) - OpenAIå‡çº§ç‰ˆï¼Œå›¾æ–‡åŒ¹é…æ›´ä¼˜
# MODEL_NAME = 'clip_vit_large_14_336.laion2b_s34b_b88k_ft_in1k'

# ã€8ã€‘MiniCPM-V 2.0 (2025 Q1) - å›½äº§è½»é‡å¤šæ¨¡æ€ï¼Œä¸­æ–‡å‹å¥½
# MODEL_NAME = 'minicpm_v_2.0_2b.clip_zh_in1k'

# ã€9ã€‘InternVL-2 (2024 Q4) - å•†æ±¤å¤šæ¨¡æ€ï¼Œä¸­æ–‡åœºæ™¯æœ€ä¼˜
# MODEL_NAME = 'internvl2_4b.clip_zh_in1k'

# ====================================================
# âš™ï¸ 2024-2025æ¨¡åž‹è‡ªåŠ¨é€‚é…é…ç½® (ç²¾å‡†åˆ†è¾¨çŽ‡/æ‰¹æ¬¡)
# ====================================================
# 2024-2025æ¨¡åž‹åˆ†è¾¨çŽ‡æ˜ å°„è¡¨ï¼ˆåŸºäºŽå®˜æ–¹æŽ¨èï¼‰
RESOLUTION_MAP = {
    # é«˜æ•ˆè½»é‡æ¨¡åž‹
    "mobilevitv4": 256,
    "efficientnetv4": 224,
    "convnextv3": 224,
    # é«˜ç²¾åº¦æ¨¡åž‹
    "eva03": 224,
    "qwen_vl_2.0": 224,
    "sam2": 256,
    # å¤šæ¨¡æ€æ¨¡åž‹
    "clip_vit_large_14_336": 336,
    "minicpm_v_2.0": 224,
    "internvl2": 224,
}

# æ™ºèƒ½åˆ†è¾¨çŽ‡åŒ¹é…
IMG_SIZE = 224  # é»˜è®¤
for key in RESOLUTION_MAP.keys():
    if key in MODEL_NAME:
        IMG_SIZE = RESOLUTION_MAP[key]
        break

# æ™ºèƒ½æ‰¹æ¬¡å¤§å° (2024-2025æ¨¡åž‹æ˜¾å­˜ä¼˜åŒ–)
batch_config = {
    "4b": 16,    # 4Bå‚æ•°é‡æ¨¡åž‹
    "large": 32, # å¤§åž‹æ¨¡åž‹
    "sam2": 16,  # SAM-2æ˜¾å­˜å ç”¨é«˜
    "base": 32,  # åŸºç¡€ç‰ˆæ¨¡åž‹
    "small": 64, # å°åž‹æ¨¡åž‹
    "atto": 64,  # è¶…è½»é‡æ¨¡åž‹
}
BATCH_SIZE = 64  # é»˜è®¤
for key, bs in batch_config.items():
    if key in MODEL_NAME:
        BATCH_SIZE = bs
        break

print(f"âš¡ 2024-2025æ¨¡åž‹é€‚é…: {MODEL_NAME} | åˆ†è¾¨çŽ‡: {IMG_SIZE}x{IMG_SIZE} | æ‰¹æ¬¡: {BATCH_SIZE}")

ROOT_TRAIN = cfg.train_dir 
ROOT_TEST = cfg.val_dir

# ====================================================
# 2. å¢žå¼ºåž‹æ•°æ®å¤„ç† (2024-2025æœ€ä½³å®žè·µ)
# ====================================================
train_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE + 32, IMG_SIZE + 32)),  # å…ˆæ”¾å¤§å†è£å‰ªï¼ˆ2024ä¸»æµï¼‰
    transforms.RandomCrop((IMG_SIZE, IMG_SIZE)),
    transforms.RandomRotation(degrees=(-15, 15)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.2),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    transforms.RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3))  # 2024å¿…åŠ å¢žå¼º
])

val_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

print(f"ðŸ“‚ æ­£åœ¨è¯»å–æ•°æ®é›†... (2024-2025 SOTAæ¨¡åž‹: {MODEL_NAME})")
train_dataset = ImageFolder(ROOT_TRAIN, transform=train_transform)
val_dataset = ImageFolder(ROOT_TEST, transform=val_transform)

class_names = train_dataset.classes
num_classes = len(class_names)
print(f"âœ… æ£€æµ‹åˆ°ç±»åˆ«: {class_names} (å…±{num_classes}ç±»)")

train_dataloader = DataLoader(
    train_dataset, 
    batch_size=BATCH_SIZE, 
    shuffle=True, 
    num_workers=0 if os.name == 'nt' else 4,  # Windowså…¼å®¹
    pin_memory=True  # 2024ä¼˜åŒ–ï¼šGPUä¼ è¾“åŠ é€Ÿ
)
val_dataloader = DataLoader(
    val_dataset, 
    batch_size=BATCH_SIZE, 
    shuffle=False, 
    num_workers=0 if os.name == 'nt' else 4,
    pin_memory=True
)

train_data_size = len(train_dataset)
val_data_size = len(val_dataset)
# 2024è®¾å¤‡é€‚é…ï¼šä¼˜å…ˆCUDAï¼Œå…¶æ¬¡MPSï¼ˆMacï¼‰ï¼Œæœ€åŽCPU
device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
print(f"ðŸ’» è®­ç»ƒè®¾å¤‡: {device}")

# ====================================================
# 3. ðŸš€ 2024-2025æ¨¡åž‹æ™ºèƒ½åŠ è½½ (å¢žå¼ºå®¹é”™)
# ====================================================
print(f"ðŸš€ æ­£åœ¨åŠ è½½2024-2025 SOTAæ¨¡åž‹: {MODEL_NAME}...")

model = None
# å¤šç­–ç•¥åŠ è½½ (é€‚é…2024æ–°ç‰ˆæ¨¡åž‹)
load_strategies = [
    # ç­–ç•¥1ï¼šå¸¦å°ºå¯¸+é¢„è®­ç»ƒï¼ˆå®˜æ–¹æŽ¨èï¼‰
    lambda: timm.create_model(
        MODEL_NAME, 
        pretrained=True, 
        num_classes=num_classes, 
        img_size=IMG_SIZE
    ),
    # ç­–ç•¥2ï¼šä¸å¸¦å°ºå¯¸+é¢„è®­ç»ƒï¼ˆå…¼å®¹éƒ¨åˆ†æ¨¡åž‹ï¼‰
    lambda: timm.create_model(
        MODEL_NAME, 
        pretrained=True, 
        num_classes=num_classes
    ),
    # ç­–ç•¥3ï¼šæœ¬åœ°æƒé‡ fallbackï¼ˆé¢„è®­ç»ƒæƒé‡ä¸‹è½½å¤±è´¥æ—¶ï¼‰
    lambda: timm.create_model(
        MODEL_NAME, 
        pretrained=False, 
        num_classes=num_classes
    )
]

for idx, strategy in enumerate(load_strategies, 1):
    try:
        print(f"   å°è¯•åŠ è½½ç­–ç•¥ {idx}...")
        model = strategy()
        if model is not None:
            print(f"   âœ… ç­–ç•¥ {idx} åŠ è½½æˆåŠŸ")
            break
    except TypeError as e:
        print(f"   âš ï¸ ç­–ç•¥ {idx} ç±»åž‹é”™è¯¯: {str(e)[:100]}")
    except RuntimeError as e:
        print(f"   âš ï¸ ç­–ç•¥ {idx} è¿è¡Œæ—¶é”™è¯¯: {str(e)[:100]}")
    except Exception as e:
        print(f"   âš ï¸ ç­–ç•¥ {idx} æœªçŸ¥é”™è¯¯: {str(e)[:100]}")

if model is None:
    print(f"âŒ æ‰€æœ‰åŠ è½½ç­–ç•¥å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
    exit()

model = model.to(device)
# 2024ä¼˜åŒ–ï¼šPyTorch 2.0+ ç¼–è¯‘åŠ é€Ÿï¼ˆä»…CUDA/MPSï¼‰
if device in ['cuda', 'mps']:
    model = torch.compile(model)
print("âœ… 2024-2025 SOTAæ¨¡åž‹åŠ è½½æˆåŠŸï¼")

# ====================================================
# 4. 2024-2025ä¼˜åŒ–çš„è®­ç»ƒé…ç½®
# ====================================================
loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)  # æ ‡ç­¾å¹³æ»‘ï¼ˆ2024ä¸»æµï¼‰

# è‡ªé€‚åº”ä¼˜åŒ–å™¨é…ç½®ï¼ˆ2024å‚æ•°æœ€ä½³å®žè·µï¼‰
lr = 5e-5 if any(key in MODEL_NAME for key in ["4b", "large", "sam2"]) else 1e-4
weight_decay = 0.05 if "vit" in MODEL_NAME or "eva03" in MODEL_NAME else 0.01

optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=lr,
    weight_decay=weight_decay,
    betas=(0.9, 0.999),
    eps=1e-8
)

# 2024ä¸»æµå­¦ä¹ çŽ‡è°ƒåº¦ï¼šä½™å¼¦é€€ç«é‡å¯
scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer,
    T_0=10,  # æ¯10ä¸ªepoché‡å¯
    T_mult=2,
    eta_min=1e-6
)

# æ··åˆç²¾åº¦è®­ç»ƒï¼ˆä»…CUDAï¼‰
scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None

# ====================================================
# 5. å¢žå¼ºåž‹è®­ç»ƒå¾ªçŽ¯ (2024-2025æœ€ä½³å®žè·µ)
# ====================================================
epoch = 30
total_train_step = 0
log_dir = f"./logs_2025_{MODEL_NAME.split('.')[0]}_{num_classes}class"
writer = SummaryWriter(log_dir)
print(f"ðŸ“ è®­ç»ƒæ—¥å¿—ä¿å­˜è‡³: {log_dir}")

start_time = time.time()
best_acc = 0.0
best_f1 = 0.0
patience = 5  # æ—©åœæœºåˆ¶ï¼ˆ2024å¿…åŠ ï¼‰
no_improve = 0

for i in range(epoch):
    print(f"\n======= ðŸ“… Epoch {i+1} / {epoch} =======")

    # --- è®­ç»ƒé˜¶æ®µ ---
    model.train()
    train_bar = tqdm(train_dataloader, desc="ðŸš€ è®­ç»ƒä¸­", unit="batch", mininterval=0.5)
    epoch_train_loss = 0.0
    epoch_train_acc = 0.0

    for data in train_bar:
        imgs, targets = data
        imgs = imgs.to(device, non_blocking=True)
        targets = targets.to(device, non_blocking=True)
        
        optimizer.zero_grad()

        # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆ2024ä¼˜åŒ–ï¼šbfloat16æ›´ç¨³å®šï¼‰
        if device == 'cuda' and scaler is not None:
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                outputs = model(imgs) 
                loss = loss_fn(outputs, targets)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(imgs)
            loss = loss_fn(outputs, targets)
            loss.backward()
            optimizer.step()

        # æ‰¹æ¬¡æŒ‡æ ‡è®¡ç®—
        batch_acc = (outputs.argmax(1) == targets).float().mean().item()
        epoch_train_loss += loss.item()
        epoch_train_acc += batch_acc
        
        train_bar.set_postfix(
            loss=f"{loss.item():.4f}", 
            acc=f"{batch_acc:.4f}",
            lr=f"{optimizer.param_groups[0]['lr']:.6f}"
        )

        writer.add_scalar("train_loss_step", loss.item(), total_train_step)
        writer.add_scalar("train_acc_step", batch_acc, total_train_step)
        total_train_step += 1

    # å­¦ä¹ çŽ‡è°ƒåº¦
    scheduler.step()

    # --- éªŒè¯é˜¶æ®µ ---
    model.eval()
    total_test_loss = 0.0
    total_accuracy = 0
    all_targets = []
    all_probs = [] 
    
    val_bar = tqdm(val_dataloader, desc="âœ… éªŒè¯ä¸­", unit="batch", mininterval=0.5)

    with torch.no_grad():
        for data in val_bar:
            imgs, targets = data
            imgs = imgs.to(device, non_blocking=True)
            targets = targets.to(device, non_blocking=True)
            
            # æ··åˆç²¾åº¦æŽ¨ç†
            with torch.cuda.amp.autocast(dtype=torch.bfloat16) if device == 'cuda' else contextlib.nullcontext():
                outputs = model(imgs)
                loss = loss_fn(outputs, targets)
            
            total_test_loss += loss.item()

            # 2024ç²¾åº¦ä¼˜åŒ–ï¼šå¼ºåˆ¶float32è®¡ç®—æ¦‚çŽ‡
            probs = torch.softmax(outputs.float(), dim=1)
            all_probs.extend(probs.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
            
            accuracy = (outputs.argmax(1) == targets).sum()
            total_accuracy += accuracy

    # --- æŒ‡æ ‡è®¡ç®— ---
    all_probs = np.array(all_probs, dtype=np.float32)
    all_targets = np.array(all_targets)
    predicted_labels = np.argmax(all_probs, axis=1)

    # å…¨å±€æŒ‡æ ‡
    val_acc = total_accuracy.item() / val_data_size
    val_loss = total_test_loss / len(val_dataloader)
    train_acc_avg = epoch_train_acc / len(train_dataloader)
    train_loss_avg = epoch_train_loss / len(train_dataloader)

    # å¤šåˆ†ç±»æŒ‡æ ‡
    precision, recall, f1_score, _ = precision_recall_fscore_support(
        all_targets, predicted_labels, average='macro', zero_division=1
    )
    
    # AUCè®¡ç®—ï¼ˆå¢žå¼ºå®¹é”™ï¼‰
    auc = 0.0
    try:
        if num_classes == 2:
            auc = roc_auc_score(all_targets, all_probs[:, 1])
        else:
            auc = roc_auc_score(
                all_targets, all_probs, 
                multi_class='ovr', 
                average='macro',
                labels=np.arange(num_classes)
            )
    except Exception as e:
        print(f"âš ï¸ AUCè®¡ç®—è·³è¿‡: {str(e)[:80]}")

    # --- æ—¥å¿—è¾“å‡º ---
    print(f"\nðŸ“Š è®­ç»ƒæŒ‡æ ‡ (Epoch {i+1}):")
    print(f"   è®­ç»ƒ | Loss: {train_loss_avg:.4f} | Acc: {train_acc_avg:.4f}")
    print(f"   éªŒè¯ | Loss: {val_loss:.4f} | Acc: {val_acc:.4f}")
    print(f"   ç»¼åˆ | AUC: {auc:.4f} | F1: {f1_score:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}")
    print(f"   å­¦ä¹ çŽ‡ | å½“å‰: {optimizer.param_groups[0]['lr']:.6f}")

    # TensorBoardè®°å½•
    writer.add_scalar("train_loss_epoch", train_loss_avg, i)
    writer.add_scalar("train_acc_epoch", train_acc_avg, i)
    writer.add_scalar("val_loss_epoch", val_loss, i)
    writer.add_scalar("val_acc_epoch", val_acc, i)
    writer.add_scalar("val_auc_epoch", auc, i)
    writer.add_scalar("val_f1_epoch", f1_score, i)
    writer.add_scalar("learning_rate", optimizer.param_groups[0]['lr'], i)

    # --- æ¨¡åž‹ä¿å­˜ (2024ä¼˜åŒ–ï¼šåªä¿å­˜æƒé‡) ---
    if val_acc > best_acc or f1_score > best_f1:
        best_acc = max(val_acc, best_acc)
        best_f1 = max(f1_score, best_f1)
        no_improve = 0  # é‡ç½®æ—©åœè®¡æ•°
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        if not os.path.exists("./model_pth_2025"):
            os.makedirs("./model_pth_2025")
        
        # ä¿å­˜æƒé‡ï¼ˆèŠ‚çœç©ºé—´ï¼‰
        simple_name = MODEL_NAME.split('.')[0] 
        save_path = f"./model_pth_2025/best_model_{simple_name}_acc{best_acc:.4f}_f1{best_f1:.4f}.pth"
        torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'epoch': i+1,
            'best_acc': best_acc,
            'best_f1': best_f1,
            'class_names': class_names
        }, save_path)
        
        print(f"ðŸŒŸ æœ€ä½³æ¨¡åž‹å·²ä¿å­˜ | Acc: {best_acc:.4f} | F1: {best_f1:.4f}")
    else:
        no_improve += 1
        print(f"âš ï¸ éªŒè¯æŒ‡æ ‡æœªæå‡ ({no_improve}/{patience})")
        if no_improve >= patience:
            print(f"ðŸ›‘ æ—©åœè§¦å‘ï¼Œæœ€ä½³Acc: {best_acc:.4f} | æœ€ä½³F1: {best_f1:.4f}")
            break

# ====================================================
# 6. è®­ç»ƒæ€»ç»“ (2024-2025å¢žå¼º)
# ====================================================
end_time = time.time()
total_time = end_time - start_time
print(f"\nðŸŽ‰ 2024-2025 SOTAæ¨¡åž‹è®­ç»ƒå®Œæˆï¼")
print(f"ðŸ“ˆ æœ€ä½³ç²¾åº¦: {best_acc:.4f} | æœ€ä½³F1: {best_f1:.4f}")
print(f"â±ï¸ æ€»è€—æ—¶: {total_time/60:.2f} åˆ†é’Ÿ (å¹³å‡ {total_time/epoch:.2f} åˆ†é’Ÿ/epoch)")
print(f"ðŸ’¾ æ¨¡åž‹ä¿å­˜è·¯å¾„: ./model_pth_2025/")

# ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
print(f"\nðŸ“‹ æœ€ç»ˆåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(all_targets, predicted_labels, target_names=class_names))

# æ··æ·†çŸ©é˜µå¯è§†åŒ–
cm = confusion_matrix(all_targets, predicted_labels)
plt.figure(figsize=(10, 8))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title(f'æ··æ·†çŸ©é˜µ ({MODEL_NAME})', fontsize=14)
plt.colorbar()
tick_marks = np.arange(num_classes)
plt.xticks(tick_marks, class_names, rotation=45)
plt.yticks(tick_marks, class_names)

# æ ‡æ³¨æ•°å€¼
thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, format(cm[i, j], 'd'),
                 ha="center", va="center",
                 color="white" if cm[i, j] > thresh else "black")

plt.ylabel('çœŸå®žæ ‡ç­¾')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.tight_layout()
plt.savefig(f"./confusion_matrix_{MODEL_NAME.split('.')[0]}.png", dpi=300)
plt.show()

writer.close()